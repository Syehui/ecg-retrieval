{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:46.777659Z",
     "iopub.status.busy": "2020-12-15T12:34:46.771405Z",
     "iopub.status.idle": "2020-12-15T12:34:46.872983Z",
     "shell.execute_reply": "2020-12-15T12:34:46.872311Z"
    },
    "papermill": {
     "duration": 0.15744,
     "end_time": "2020-12-15T12:34:46.873100",
     "exception": false,
     "start_time": "2020-12-15T12:34:46.715660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ecg-reg-best-model/test_samples.pickle\n",
      "/kaggle/input/ecg-reg-best-model/checkpoint_step1120\n",
      "/kaggle/input/ecg-reg-best-model/train_samples.pickle\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import csv\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from IPython.display import FileLink\n",
    "import gzip\n",
    "import os\n",
    "\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "\"\"\"for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        break\"\"\"\n",
    "\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input/ecg-reg-best-model'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "try:\n",
    "    for line in open('/kaggle/input/ecg-retrieval-static-integrated/running_log'):\n",
    "        print(line.strip())\n",
    "except:\n",
    "    pass\n",
    "\n",
    "count_label_dict = {82: 51162, 81: 14897, 57: 14733, 83: 11989, 46: 8344, 67: 6867, 90: 6573, 8: 6269, 61: 5412, 49: 5202, 76: 4484, 78: 4030, 80: 3649, 2: 3389, 84: 2996, 0: 2847, 75: 2826, 66: 2724, 58: 2597, 24: 2249, 51: 2104, 33: 1972, 42: 1947, 114: 1932, 86: 1886, 68: 1822, 110: 1718, 69: 1665, 62: 1409, 115: 1380, 91: 1361, 71: 1354, 17: 1324, 44: 1290, 47: 1255, 85: 1161, 60: 1105, 41: 1043, 87: 980, 94: 921, 10: 802, 88: 738, 19: 682, 35: 668, 116: 644, 118: 644, 98: 558, 121: 552, 7: 428, 100: 413, 106: 410, 63: 392, 95: 367, 70: 353, 6: 324, 103: 315, 104: 310, 64: 300, 15: 278, 117: 276, 55: 276, 122: 269, 77: 260, 107: 251, 14: 247, 32: 220, 48: 202, 27: 199, 13: 189, 119: 184, 74: 177, 21: 161, 34: 156, 52: 110, 26: 106, 96: 103, 113: 99, 16: 94, 120: 92, 79: 83, 109: 81, 108: 74, 43: 72, 11: 69, 1: 68, 23: 67, 9: 60, 12: 52, 45: 47, 72: 46, 112: 46, 101: 44, 38: 32, 102: 28, 93: 23, 4: 19, 111: 14, 59: 14, 73: 14, 29: 13, 89: 12, 65: 10, 105: 7, 40: 6, 97: 5, 31: 5, 39: 4, 5: 2, 99: 2, 25: 2, 50: 1, 36: 1, 20: 1}\n",
    "num_test = {'patient001': 2, 'patient002': 1, 'patient003': 1, 'patient004': 2, 'patient005': 5, 'patient006': 3, 'patient007': 4, 'patient008': 3, 'patient009': 1, 'patient010': 3, 'patient011': 3, 'patient012': 2, 'patient013': 3, 'patient014': 3, 'patient015': 3, 'patient016': 3, 'patient017': 4, 'patient018': 3, 'patient019': 3, 'patient020': 3, 'patient021': 3, 'patient022': 3, 'patient023': 4, 'patient024': 4, 'patient025': 3, 'patient026': 2, 'patient027': 3, 'patient028': 3, 'patient029': 3, 'patient030': 4, 'patient031': 4, 'patient032': 4, 'patient033': 4, 'patient034': 4, 'patient035': 4, 'patient036': 3, 'patient037': 2, 'patient038': 3, 'patient039': 3, 'patient040': 4, 'patient041': 4, 'patient042': 4, 'patient043': 3, 'patient044': 4, 'patient045': 4, 'patient046': 4, 'patient047': 3, 'patient048': 4, 'patient049': 4, 'patient050': 4, 'patient051': 4, 'patient052': 1, 'patient053': 1, 'patient054': 4, 'patient055': 1, 'patient056': 1, 'patient057': 1, 'patient058': 1, 'patient059': 1, 'patient060': 1, 'patient061': 1, 'patient062': 1, 'patient063': 1, 'patient064': 1, 'patient065': 4, 'patient066': 3, 'patient067': 3, 'patient068': 1, 'patient069': 4, 'patient070': 1, 'patient071': 1, 'patient072': 4, 'patient073': 4, 'patient074': 4, 'patient075': 4, 'patient076': 4, 'patient077': 4, 'patient078': 4, 'patient079': 4, 'patient080': 4, 'patient081': 4, 'patient082': 4, 'patient083': 4, 'patient084': 4, 'patient085': 4, 'patient086': 1, 'patient087': 3, 'patient088': 4, 'patient089': 4, 'patient090': 4, 'patient091': 4, 'patient092': 4, 'patient093': 5, 'patient094': 4, 'patient095': 4, 'patient096': 4, 'patient097': 4, 'patient098': 4, 'patient099': 4, 'patient100': 3, 'patient101': 3, 'patient102': 1, 'patient103': 1, 'patient104': 1, 'patient105': 1, 'patient106': 1, 'patient107': 1, 'patient108': 1, 'patient109': 1, 'patient110': 1, 'patient111': 1, 'patient112': 1, 'patient113': 1, 'patient114': 1, 'patient115': 1, 'patient116': 1, 'patient117': 2, 'patient118': 1, 'patient119': 1, 'patient120': 1, 'patient121': 1, 'patient122': 1, 'patient123': 1, 'patient125': 1, 'patient126': 1, 'patient127': 2, 'patient128': 1, 'patient129': 1, 'patient130': 1, 'patient131': 1, 'patient133': 1, 'patient135': 1, 'patient136': 1, 'patient137': 1, 'patient138': 1, 'patient139': 1, 'patient140': 1, 'patient141': 1, 'patient142': 1, 'patient143': 1, 'patient144': 1, 'patient145': 1, 'patient146': 1, 'patient147': 1, 'patient148': 1, 'patient149': 1, 'patient150': 1, 'patient151': 1, 'patient152': 1, 'patient153': 1, 'patient154': 1, 'patient155': 1, 'patient156': 1, 'patient157': 1, 'patient158': 2, 'patient159': 1, 'patient160': 1, 'patient162': 1, 'patient163': 1, 'patient164': 1, 'patient165': 2, 'patient166': 1, 'patient167': 1, 'patient168': 1, 'patient169': 2, 'patient170': 1, 'patient171': 1, 'patient172': 1, 'patient173': 1, 'patient174': 3, 'patient175': 1, 'patient176': 1, 'patient177': 1, 'patient178': 1, 'patient179': 1, 'patient180': 8, 'patient181': 1, 'patient182': 1, 'patient183': 1, 'patient184': 1, 'patient185': 1, 'patient186': 1, 'patient187': 1, 'patient188': 1, 'patient189': 1, 'patient190': 1, 'patient191': 1, 'patient192': 1, 'patient193': 1, 'patient194': 1, 'patient195': 1, 'patient196': 1, 'patient197': 2, 'patient198': 2, 'patient199': 1, 'patient200': 1, 'patient201': 2, 'patient202': 2, 'patient203': 1, 'patient204': 1, 'patient205': 1, 'patient206': 1, 'patient207': 1, 'patient208': 2, 'patient209': 1, 'patient210': 1, 'patient211': 1, 'patient212': 1, 'patient213': 1, 'patient214': 1, 'patient215': 1, 'patient216': 1, 'patient217': 1, 'patient218': 1, 'patient219': 1, 'patient220': 1, 'patient221': 1, 'patient222': 1, 'patient223': 2, 'patient224': 1, 'patient225': 1, 'patient226': 1, 'patient227': 1, 'patient228': 1, 'patient229': 2, 'patient230': 1, 'patient231': 1, 'patient232': 1, 'patient233': 6, 'patient234': 1, 'patient235': 1, 'patient236': 3, 'patient237': 1, 'patient238': 1, 'patient239': 1, 'patient240': 1, 'patient241': 2, 'patient242': 1, 'patient243': 1, 'patient244': 1, 'patient245': 2, 'patient246': 1, 'patient247': 1, 'patient248': 1, 'patient249': 1, 'patient250': 1, 'patient251': 3, 'patient252': 1, 'patient253': 1, 'patient254': 1, 'patient255': 1, 'patient256': 1, 'patient257': 1, 'patient258': 1, 'patient259': 1, 'patient260': 1, 'patient261': 1, 'patient262': 1, 'patient263': 1, 'patient264': 1, 'patient265': 1, 'patient266': 1, 'patient267': 1, 'patient268': 1, 'patient269': 1, 'patient270': 1, 'patient271': 1, 'patient272': 1, 'patient273': 1, 'patient274': 1, 'patient275': 1, 'patient276': 1, 'patient277': 1, 'patient278': 3, 'patient279': 4, 'patient280': 1, 'patient281': 1, 'patient282': 1, 'patient283': 1, 'patient284': 3, 'patient285': 1, 'patient286': 1, 'patient287': 2, 'patient288': 1, 'patient289': 1, 'patient290': 1, 'patient291': 1, 'patient292': 2, 'patient293': 2, 'patient294': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:46.931659Z",
     "iopub.status.busy": "2020-12-15T12:34:46.928174Z",
     "iopub.status.idle": "2020-12-15T12:34:46.946932Z",
     "shell.execute_reply": "2020-12-15T12:34:46.945859Z"
    },
    "papermill": {
     "duration": 0.052638,
     "end_time": "2020-12-15T12:34:46.947064",
     "exception": false,
     "start_time": "2020-12-15T12:34:46.894426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_class  294\n"
     ]
    }
   ],
   "source": [
    "\"\"\"data schema:\n",
    "key, base64_data, base64_label, features\n",
    "\"\"\"\n",
    "import multiprocessing\n",
    "\n",
    "path = {}\n",
    "\n",
    "download_and_use_pretrained_model = False\n",
    "\n",
    "is_load_from_pretrain = False\n",
    "\n",
    "path['pretrain_model_path'] = '../input/50ep1e30775/50ep1e30.775'\n",
    "\n",
    "shuffle = True\n",
    "is_gzip_file = False\n",
    "valid_ratio = 0.05  #5948 samples\n",
    "test_ratio = 0.05   #5948 samples\n",
    "train_ratio = 1 - valid_ratio - test_ratio   #107064 samples\n",
    "worker_num = multiprocessing.cpu_count()\n",
    "#worker_num = 8\n",
    "\n",
    "topk = 100\n",
    "feature_index = 0\n",
    "label_index = 1\n",
    "\n",
    "epochs = 28\n",
    "batch_size = 128\n",
    "#learning_rate = 5e-4\n",
    "learning_rate = 1e-3\n",
    "is_warmup_decay = 1\n",
    "warmup_ratio = 0.1\n",
    "threshold = 0.5\n",
    "sigmoid_input = False\n",
    "\n",
    "use_optimizer = \"adam\"\n",
    "use_l2_penalty = False\n",
    "adam_epsilon = 1e-8\n",
    "\n",
    "use_float32 = False\n",
    "\n",
    "use_selected_label = False\n",
    "rhythm_labels = [80,81,82,8,83]\n",
    "top_10_labels = [82, 81, 57, 83, 46, 67, 90, 8, 61, 49]\n",
    "top_30_labels = [82, 81, 57, 83, 46, 67, 90, 8, 61, 49, 76, 78, 80, 2, 84, 0, 75, 66, 58, 24, 51, 33, 42, 114, 86, 68, 110, 69, 62, 115]\n",
    "select_label = rhythm_labels\n",
    "\n",
    "use_sto_features = False\n",
    "use_reranking = True\n",
    "\n",
    "leads = 12\n",
    "num_samples = 5181\n",
    "num_class = 294\n",
    "feature_size = 242\n",
    "\n",
    "path['use_resnet'] = 'resnet34'\n",
    "    \n",
    "if use_selected_label: \n",
    "    num_class = len(select_label)\n",
    "print(\"num_class \", num_class)\n",
    "\n",
    "steps_per_epochs = int(num_samples/batch_size)\n",
    "num_steps = steps_per_epochs * epochs\n",
    "save_step = int(num_steps/10)\n",
    "decay_step = [int(num_steps*0.6), int(num_steps*0.8)]\n",
    "warmup_step = max(2*steps_per_epochs, int(num_steps/10))\n",
    "#evaluate_step = int(num_steps/10)\n",
    "evaluate_step = steps_per_epochs * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:46.994953Z",
     "iopub.status.busy": "2020-12-15T12:34:46.994096Z",
     "iopub.status.idle": "2020-12-15T12:34:46.998925Z",
     "shell.execute_reply": "2020-12-15T12:34:46.998266Z"
    },
    "papermill": {
     "duration": 0.031558,
     "end_time": "2020-12-15T12:34:46.999037",
     "exception": false,
     "start_time": "2020-12-15T12:34:46.967479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1120"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:47.053520Z",
     "iopub.status.busy": "2020-12-15T12:34:47.052812Z",
     "iopub.status.idle": "2020-12-15T12:34:47.670987Z",
     "shell.execute_reply": "2020-12-15T12:34:47.671960Z"
    },
    "papermill": {
     "duration": 0.650398,
     "end_time": "2020-12-15T12:34:47.672139",
     "exception": false,
     "start_time": "2020-12-15T12:34:47.021741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load train file list, valid file list and test file list\n",
    "import os\n",
    "\n",
    "path['input_data'] = ['/kaggle/input/prepare-ecg-reg-data']\n",
    "\n",
    "def get_train_val_test_list(paths):\n",
    "    files = []\n",
    "    for dir_add in paths:\n",
    "        f_adds = os.listdir(dir_add)\n",
    "        f_adds = sorted(f_adds)\n",
    "        for i,f_add in enumerate(f_adds.copy()):\n",
    "            if '.part' not in f_add:\n",
    "                f_adds.remove(f_add)\n",
    "            else:\n",
    "                f_add = dir_add + '/' + f_add\n",
    "                files.append(f_add)\n",
    "    num_samples = len(files)\n",
    "    num_valids = 0\n",
    "    num_tests = 543\n",
    "    num_trains = num_samples - num_valids - num_tests\n",
    "    #return train_list, valid_list, test_list\n",
    "    return files[num_tests+num_valids:], files[num_tests:num_tests+num_valids], files[:num_tests]\n",
    "    \n",
    "path['train_list'], path['valid_list'], path['test_list'] = get_train_val_test_list(path['input_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:47.720130Z",
     "iopub.status.busy": "2020-12-15T12:34:47.719465Z",
     "iopub.status.idle": "2020-12-15T12:34:47.723523Z",
     "shell.execute_reply": "2020-12-15T12:34:47.724120Z"
    },
    "papermill": {
     "duration": 0.030978,
     "end_time": "2020-12-15T12:34:47.724259",
     "exception": false,
     "start_time": "2020-12-15T12:34:47.693281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5181\n",
      "0\n",
      "543\n"
     ]
    }
   ],
   "source": [
    "print(len(path['train_list']))\n",
    "print(len(path['valid_list']))\n",
    "print(len(path['test_list']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:47.771135Z",
     "iopub.status.busy": "2020-12-15T12:34:47.770438Z",
     "iopub.status.idle": "2020-12-15T12:34:48.709351Z",
     "shell.execute_reply": "2020-12-15T12:34:48.708774Z"
    },
    "papermill": {
     "duration": 0.963615,
     "end_time": "2020-12-15T12:34:48.709463",
     "exception": false,
     "start_time": "2020-12-15T12:34:47.745848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set size:  5181\n",
      "valid set size:  0\n",
      "test set size:  543\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gzip\n",
    "from torch.utils import data\n",
    "import base64\n",
    "import linecache\n",
    "\n",
    "class MyDataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "        \n",
    "    def __init__(self, paths, out_feature=True):\n",
    "        \n",
    "        self.files = paths\n",
    "            \n",
    "        self.out_feature = out_feature\n",
    "        \n",
    "    def parse_line(self, line, f_add):\n",
    "        \n",
    "        id, name, segment, data, label, features = line.strip().split('\\t')\n",
    "        \n",
    "        index_data = base64.b64decode(data)\n",
    "        index_data = np.frombuffer(index_data).reshape(12,-1)\n",
    "        \n",
    "        index_label = int(id[-3:])-1\n",
    "        \n",
    "        sample = {}\n",
    "        sample['id'] = id\n",
    "        sample['file_name'] = f_add\n",
    "        sample['index_name'] = name\n",
    "        sample['segment'] = segment\n",
    "        sample['index_data'] = index_data\n",
    "        sample['index_label'] = index_label\n",
    "        sample['features'] = features\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        \n",
    "        f_add = self.files[index]\n",
    "        with open(f_add, 'r') as f:\n",
    "            line = f.read()\n",
    "            sample = self.parse_line(line, f_add)\n",
    "    \n",
    "        return sample\n",
    "\n",
    "train_set = MyDataset(path['train_list'],out_feature=True)\n",
    "valid_set = MyDataset(path['valid_list'],out_feature=True)\n",
    "test_set = MyDataset(path['test_list'],out_feature=True)\n",
    "train_generator = data.DataLoader(train_set, batch_size=batch_size, shuffle=shuffle, num_workers=worker_num)\n",
    "valid_generator = data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=worker_num)\n",
    "test_generator = data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=worker_num)\n",
    "print(\"train set size: \", train_set.__len__())\n",
    "print(\"valid set size: \", valid_set.__len__())\n",
    "print(\"test set size: \", test_set.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:48.775321Z",
     "iopub.status.busy": "2020-12-15T12:34:48.764721Z",
     "iopub.status.idle": "2020-12-15T12:34:48.820453Z",
     "shell.execute_reply": "2020-12-15T12:34:48.819917Z"
    },
    "papermill": {
     "duration": 0.089034,
     "end_time": "2020-12-15T12:34:48.820596",
     "exception": false,
     "start_time": "2020-12-15T12:34:48.731562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class BasicModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicModule, self).__init__()\n",
    "\n",
    "    def init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.01)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        \n",
    "def activation(act_type='prelu'):\n",
    "    if act_type == 'prelu':\n",
    "        act = nn.PReLU()\n",
    "    elif act_type == 'relu':\n",
    "        act = nn.ReLU(inplace=True)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return act\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv1d(in_planes, out_planes, kernel_size=7, stride=stride,\n",
    "                     padding=3, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.relu = activation('relu')\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "        #self.dropout = nn.Dropout(.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(inplanes, planes, kernel_size=7, bias=False, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(planes)\n",
    "        self.conv2 = nn.Conv1d(planes, planes, kernel_size=11, stride=stride,\n",
    "                               padding=5, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(planes)\n",
    "        self.conv3 = nn.Conv1d(planes, planes * 4, kernel_size=7, bias=False, padding=3)\n",
    "        self.bn3 = nn.BatchNorm1d(planes * 4)\n",
    "        self.relu = activation('relu')\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(BasicModule):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=num_class):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(leads, 64, kernel_size=15, stride=2, padding=7,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn_sto = nn.BatchNorm1d(feature_size)\n",
    "        self.relu = activation('relu')\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.use_sto = use_sto_features\n",
    "        \n",
    "        self.fc_size = 512\n",
    "        if self.use_sto:\n",
    "            self.fc_size += 64\n",
    "            \n",
    "        print(\"block.expansion\",block.expansion)\n",
    "        self.fc1 = nn.Linear(self.fc_size * block.expansion, int(num_classes))\n",
    "        self.fc2 = nn.Linear(int(num_classes), num_classes)\n",
    "        self.fc_sto = nn.Linear(feature_size, 64)\n",
    "        self.dropout = nn.Dropout(.2)\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc_sto.weight)\n",
    "        # V1\n",
    "        #self.avgpool = nn.AdaptiveAvgPool1d(2)\n",
    "        #self.fc = nn.Linear(2 * 512 * block.expansion, num_classes)\n",
    "        # v2\n",
    "        #self.conv2 = nn.Conv1d(2048, 64, kernel_size=1, stride=1, padding=0,\n",
    "        #                       bias=False)\n",
    "        #self.bn2 = nn.BatchNorm1d(64)\n",
    "        #self.avgpool = nn.AdaptiveAvgPool1d(8)\n",
    "        #self.fc = nn.Linear(64 * 8, num_classes)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.init()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv1d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, data, features):\n",
    "        x01 = self.conv1(data)\n",
    "        x02 = self.bn1(x01)\n",
    "        x03 = self.relu(x02)\n",
    "        x0 = self.maxpool(x03)\n",
    "\n",
    "        x1 = self.layer1(x0)\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "\n",
    "        x_final = self.avgpool(x4)\n",
    "        x_final = x_final.view(x_final.size(0), -1)\n",
    "\n",
    "        #sex = sex.unsqueeze(1)\n",
    "        #inp = torch.cat([x, age, sex], 1)\n",
    "        \n",
    "        assert not (torch.isnan(features).any() or features.eq(float('inf')).any() or features.eq(float('-inf')).any())\n",
    "        \n",
    "        if self.use_sto:\n",
    "            #features = self.bn_sto(features)\n",
    "            features = self.fc_sto(features)\n",
    "            x_final = torch.cat([x_final, features], 1)\n",
    "\n",
    "        fc1_out = self.fc1(self.dropout(x_final))\n",
    "        #fc2_out = self.fc2(self.dropout(self.relu(fc1_out)))\n",
    "        \n",
    "        #x = self.sigmoid(x)\n",
    "        return fc1_out, x_final\n",
    "        #return fc2_out, x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:48.897354Z",
     "iopub.status.busy": "2020-12-15T12:34:48.890211Z",
     "iopub.status.idle": "2020-12-15T12:34:50.058122Z",
     "shell.execute_reply": "2020-12-15T12:34:50.057514Z"
    },
    "papermill": {
     "duration": 1.215182,
     "end_time": "2020-12-15T12:34:50.058236",
     "exception": false,
     "start_time": "2020-12-15T12:34:48.843054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train on:  cpu\n",
      "n_gpu:  0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from torch.utils import data\n",
    "import gc\n",
    "from torch.optim import Adam, AdamW, Adagrad, SGD, RMSprop\n",
    "from time import time\n",
    "from random import random\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "print(\"train on: \", device)\n",
    "print(\"n_gpu: \", n_gpu)\n",
    "\n",
    "\n",
    "def warmup_decap(current_step, num_steps, warmup_step, decay_step, lr_decay_ratio=0.1):\n",
    "    #current_step < warmup_step\n",
    "    if current_step < warmup_step:\n",
    "        return current_step * 1.0 / warmup_step\n",
    "    #curret_step > decay_step\n",
    "    for i,step in enumerate(list(reversed(decay_step))):\n",
    "        if current_step > step:\n",
    "            return pow(lr_decay_ratio, len(decay_step) - i)\n",
    "    #warmup_step < curret_step < decay_step\n",
    "    return 1.0\n",
    "\n",
    "#计算F1score\n",
    "def calc_f1(y_true, y_pre, threshold=0.5):\n",
    "    y_true = y_true.view(-1).cpu().detach().numpy().astype(np.int)\n",
    "    y_pre = y_pre.view(-1).cpu().detach().numpy() > threshold\n",
    "    return f1_score(y_true, y_pre)\n",
    "\n",
    "def load_from_pretrain(model, model_path, optimizer):\n",
    "    print('load from ', model_path)\n",
    "    #load pre_trained dict\n",
    "    model_dict = model.state_dict()\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path)\n",
    "    except:\n",
    "        checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    \n",
    "    try:\n",
    "        pretrained_dict = checkpoint[\"net\"]\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])  # load optimizer\n",
    "        print('optimizer loaded')\n",
    "    except: \n",
    "        pretrained_dict = checkpoint\n",
    "        \n",
    "    # 1. filter out unnecessary keys\n",
    "    #pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if\n",
    "                           (k in model_dict) and (model_dict[k].shape == pretrained_dict[k].shape)}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict) \n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict, strict=False)\n",
    "    print('pretrain_model loaded')\n",
    "    \n",
    "    return model, optimizer\n",
    "    \n",
    "def evaluate(model, valid_generator):\n",
    "    print('evaluating...')\n",
    "    model.eval()\n",
    "    v_loss = 0.\n",
    "    f1_all = 0.\n",
    "    num_batches_per_epoch = 0\n",
    "    samples = {}\n",
    "    samples['file_name'] = []\n",
    "    samples['id'] = []\n",
    "    samples['index_name'] = []\n",
    "    samples['index_data'] = []\n",
    "    samples['index_label'] = []\n",
    "    samples['segment'] = []\n",
    "    samples['sto_features'] = []\n",
    "    samples['dynamic_features'] = []\n",
    "    with torch.no_grad():\n",
    "        for batch_samples in tqdm(valid_generator):\n",
    "                \n",
    "            batch_data = batch_samples[\"index_data\"].type(torch.FloatTensor).to(device)\n",
    "            #batch_label = batch_samples[\"index_label\"].type(torch.FloatTensor).to(device)\n",
    "            batch_label = batch_samples[\"index_label\"].type(torch.cuda.LongTensor).to(device)\n",
    "            try:\n",
    "                batch_feature = batch_samples[\"features\"].type(torch.FloatTensor).to(device)\n",
    "            except:\n",
    "                batch_feature = torch.FloatTensor([0]).to(device)\n",
    "            \n",
    "            logit, batch_feature = model(batch_data, batch_feature)\n",
    "            \n",
    "            samples['index_name'].extend(batch_samples['index_name'])\n",
    "            samples['file_name'].extend(batch_samples['file_name'])\n",
    "            samples['id'].extend(batch_samples['id'])\n",
    "            samples['segment'].extend(batch_samples['segment'])\n",
    "            #samples['index_data'].extend(batch_samples['index_data'])\n",
    "            samples['index_label'].extend(batch_samples[\"index_label\"].cpu().detach().numpy())\n",
    "            samples['sto_features'].extend(batch_samples['features'])\n",
    "            samples['dynamic_features'].extend(batch_feature.cpu().detach().numpy())\n",
    "            \n",
    "            loss = loss_fct(logit, batch_label)\n",
    "\n",
    "            num_batches_per_epoch += 1\n",
    "            \n",
    "            v_loss += loss.item()\n",
    "        \n",
    "            del logit, loss\n",
    "            #break\n",
    "    print(\"valid set's batch loss \", v_loss/num_batches_per_epoch)\n",
    "    return v_loss, samples\n",
    "\n",
    "#loss_fct = nn.BCEWithLogitsLoss()\n",
    "loss_fct = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:50.132869Z",
     "iopub.status.busy": "2020-12-15T12:34:50.127625Z",
     "iopub.status.idle": "2020-12-15T12:34:50.571779Z",
     "shell.execute_reply": "2020-12-15T12:34:50.572314Z"
    },
    "papermill": {
     "duration": 0.491438,
     "end_time": "2020-12-15T12:34:50.572459",
     "exception": false,
     "start_time": "2020-12-15T12:34:50.081021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block.expansion 1\n",
      "adam without w is used\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.model_zoo as model_zoo\n",
    "model_urls = {\n",
    "    'basic_path': 'https://download.pytorch.org/models/',\n",
    "    'resnet18': 'resnet18-5c106cde.pth',\n",
    "    'resnet34': 'resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'resnet50-19c8e357.pth',\n",
    "    'resnet101': 'resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'resnet152-b121ed1d.pth',\n",
    "}\n",
    "\n",
    "def get_optimizer(model, opt=\"adam\"):\n",
    "    #choose optimizer\n",
    "    if opt == \"adamw\":\n",
    "        print(\"adamW is used\")\n",
    "        optimizer = AdamW(params=model.parameters(), lr=learning_rate)\n",
    "        return optimizer\n",
    "    elif opt == \"adam\":\n",
    "        if use_l2_penalty:\n",
    "            print(\"adam with w is used\")\n",
    "            optimizer = Adam(params=model.parameters(), lr=learning_rate, weight_decay=adam_epsilon)\n",
    "            return optimizer\n",
    "        else:\n",
    "            print(\"adam without w is used\")\n",
    "            optimizer = Adam(params=model.parameters(), lr=learning_rate)\n",
    "            return optimizer\n",
    "\n",
    "def resnet18(pretrained=False, **kwargs):\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    optimizer = get_optimizer(model, use_optimizer)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet18']))\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def resnet34(pretrained=False, **kwargs):\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    optimizer = get_optimizer(model, use_optimizer)\n",
    "    if pretrained:\n",
    "        !wget https://download.pytorch.org/models/resnet34-333f7ec4.pth\n",
    "        model, optimizer = load_from_pretrain(model, model_urls['resnet34'], optimizer)\n",
    "        !rm resnet34-333f7ec4.pth\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def resnet50(pretrained=False, **kwargs):\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    optimizer = get_optimizer(model, use_optimizer)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def resnet101(pretrained=False, **kwargs):\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    optimizer = get_optimizer(model, use_optimizer)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet101']))\n",
    "    return model, optimizer\n",
    "\n",
    "\n",
    "def resnet152(pretrained=False, **kwargs):\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    optimizer = get_optimizer(model, use_optimizer)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['resnet152']))\n",
    "    return model, optimizer\n",
    "\n",
    "if path['use_resnet'] == 'resnet18': \n",
    "    model, optimizer = resnet18(download_and_use_pretrained_model)\n",
    "elif path['use_resnet'] == 'resnet34':\n",
    "    model, optimizer = resnet34(download_and_use_pretrained_model)\n",
    "elif path['use_resnet'] == 'resnet50':\n",
    "    model, optimizer = resnet50(download_and_use_pretrained_model)\n",
    "elif path['use_resnet'] == 'resnet101':\n",
    "    model, optimizer = resnet101(download_and_use_pretrained_model)\n",
    "elif path['use_resnet'] == 'resnet152':\n",
    "    model, optimizer = resnet152(download_and_use_pretrained_model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:50.631125Z",
     "iopub.status.busy": "2020-12-15T12:34:50.630093Z",
     "iopub.status.idle": "2020-12-15T12:34:50.651614Z",
     "shell.execute_reply": "2020-12-15T12:34:50.650920Z"
    },
    "papermill": {
     "duration": 0.051819,
     "end_time": "2020-12-15T12:34:50.651748",
     "exception": false,
     "start_time": "2020-12-15T12:34:50.599929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all:  16854548  trainable:  16854548\n",
      "BEGIN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model.train()\\ncurrent_step = 0\\nfor epoch in range(epochs):\\n    start = time()\\n    train_loss = 0.0\\n    f1_all_train = 0.\\n    num_batches_per_epoch = 0\\n    for batch_samples in tqdm(train_generator):\\n        \\n        batch_data = batch_samples[\"index_data\"].to(device).type(torch.cuda.FloatTensor)\\n        batch_label = batch_samples[\"index_label\"].to(device).type(torch.cuda.LongTensor)\\n        try:\\n            batch_feature = batch_samples[\"features\"].type(torch.FloatTensor).to(device)\\n        except:\\n            batch_feature = torch.FloatTensor([0]).to(device)\\n        \\n        if sigmoid_input:\\n            batch_data = torch.sigmoid(batch_data)\\n            \\n        optimizer.zero_grad()   #u don\\'t want to forget this\\n        \\n        logits, _ = model(batch_data, batch_feature)\\n        \\n        #print(logits)\\n        \\n        loss = loss_fct(logits, batch_label)\\n        #print(loss)\\n            \\n        train_loss += loss.item()\\n\\n        loss.backward()\\n        optimizer.step()\\n\\n        current_step += 1\\n        num_batches_per_epoch += 1\\n        \\n        #tune learning rate\\n        if is_warmup_decay:\\n            lr_this_step = learning_rate * warmup_decap(current_step, num_steps, warmup_step, decay_step)\\n            for param_group in optimizer.param_groups:\\n                param_group[\\'lr\\'] = lr_this_step\\n        \\n        #evaluation\\n        if current_step % evaluate_step == 0:\\n            \\n            print(\\'current step: \\', current_step)\\n            print(\\'current lr: \\', lr_this_step)\\n            \\n            save_path = \\'checkpoint_step\\'+(str(current_step))\\n            torch.save({\\n                \"step\": current_step,\\n                \"loss\": train_loss/num_batches_per_epoch,\\n                \"net\": model.state_dict(),\\n                \"optimizer\": optimizer.state_dict()\\n            }, save_path)\\n            \\n            dev_loss,_ = evaluate(model, valid_generator)\\n            model.train()   #very important\\n            print(\\'model saved, save step: \\', current_step)\\n            \\n            path[\\'best_model\\'] = save_path\\n        \\n        del logits, loss, batch_data, batch_label\\n        \\n    end = time()\\n    print(\\'epoch {}, batch loss {:.3f} - took {} sec\\'.format(epoch+1, train_loss/num_batches_per_epoch, int(end-start)))\\n    #print(\\'train f1_score: \\', f1_all_train/num_batches_per_epoch)\\n    start = end\\n    gc.collect()'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if use_float32 == False:\n",
    "#    model = model.half()\n",
    "\n",
    "if is_load_from_pretrain == True and path['pretrain_model_path']:\n",
    "    model, optimizer = load_from_pretrain(model, path['pretrain_model_path'], optimizer)\n",
    "\n",
    "#count #parameter\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"all: \", total_params, \" trainable: \",trainable_params)\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "print('BEGIN:')\n",
    "\n",
    "\"\"\"model.train()\n",
    "current_step = 0\n",
    "for epoch in range(epochs):\n",
    "    start = time()\n",
    "    train_loss = 0.0\n",
    "    f1_all_train = 0.\n",
    "    num_batches_per_epoch = 0\n",
    "    for batch_samples in tqdm(train_generator):\n",
    "        \n",
    "        batch_data = batch_samples[\"index_data\"].to(device).type(torch.cuda.FloatTensor)\n",
    "        batch_label = batch_samples[\"index_label\"].to(device).type(torch.cuda.LongTensor)\n",
    "        try:\n",
    "            batch_feature = batch_samples[\"features\"].type(torch.FloatTensor).to(device)\n",
    "        except:\n",
    "            batch_feature = torch.FloatTensor([0]).to(device)\n",
    "        \n",
    "        if sigmoid_input:\n",
    "            batch_data = torch.sigmoid(batch_data)\n",
    "            \n",
    "        optimizer.zero_grad()   #u don't want to forget this\n",
    "        \n",
    "        logits, _ = model(batch_data, batch_feature)\n",
    "        \n",
    "        #print(logits)\n",
    "        \n",
    "        loss = loss_fct(logits, batch_label)\n",
    "        #print(loss)\n",
    "            \n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_step += 1\n",
    "        num_batches_per_epoch += 1\n",
    "        \n",
    "        #tune learning rate\n",
    "        if is_warmup_decay:\n",
    "            lr_this_step = learning_rate * warmup_decap(current_step, num_steps, warmup_step, decay_step)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "        \n",
    "        #evaluation\n",
    "        if current_step % evaluate_step == 0:\n",
    "            \n",
    "            print('current step: ', current_step)\n",
    "            print('current lr: ', lr_this_step)\n",
    "            \n",
    "            save_path = 'checkpoint_step'+(str(current_step))\n",
    "            torch.save({\n",
    "                \"step\": current_step,\n",
    "                \"loss\": train_loss/num_batches_per_epoch,\n",
    "                \"net\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict()\n",
    "            }, save_path)\n",
    "            \n",
    "            dev_loss,_ = evaluate(model, valid_generator)\n",
    "            model.train()   #very important\n",
    "            print('model saved, save step: ', current_step)\n",
    "            \n",
    "            path['best_model'] = save_path\n",
    "        \n",
    "        del logits, loss, batch_data, batch_label\n",
    "        \n",
    "    end = time()\n",
    "    print('epoch {}, batch loss {:.3f} - took {} sec'.format(epoch+1, train_loss/num_batches_per_epoch, int(end-start)))\n",
    "    #print('train f1_score: ', f1_all_train/num_batches_per_epoch)\n",
    "    start = end\n",
    "    gc.collect()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:50.705234Z",
     "iopub.status.busy": "2020-12-15T12:34:50.704240Z",
     "iopub.status.idle": "2020-12-15T12:34:51.410699Z",
     "shell.execute_reply": "2020-12-15T12:34:51.411248Z"
    },
    "papermill": {
     "duration": 0.735019,
     "end_time": "2020-12-15T12:34:51.411395",
     "exception": false,
     "start_time": "2020-12-15T12:34:50.676376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples loaded\n",
      "test samples loaded\n"
     ]
    }
   ],
   "source": [
    "#get feature for knn retrieval\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_features(path, train_generator, valid_generator, test_generator, optimizer):\n",
    "    try:\n",
    "        model, optimizer = load_from_pretrain(model, path['best_model'], optimizer)\n",
    "    except:\n",
    "        model, optimizer = resnet34(download_and_use_pretrained_model)\n",
    "        if(path.get('best_model',0) != 0):\n",
    "            model, optimizer = load_from_pretrain(model, path['best_model'], optimizer)\n",
    "    model = model.to(device)\n",
    "    train_loss,train_samples = evaluate(model, train_generator)\n",
    "    test_loss,test_samples = evaluate(model, test_generator)\n",
    "    \n",
    "    return train_samples,test_samples\n",
    "\n",
    "path['best_model'] = 'checkpoint_step1120'\n",
    "#path['best_model'] = '../input/50ep1e30775/50ep1e30.775'\n",
    "#path['best_model'] = '/kaggle/input/ecg-retrieval-static-integrated/checkpoint_step41805'\n",
    "\n",
    "load_samples = True\n",
    "save_samples = False\n",
    "if load_samples:\n",
    "    with open('../input/ecg-reg-best-model/train_samples.pickle', 'rb') as handle:\n",
    "        train_samples = pickle.load(handle)\n",
    "    print(\"train samples loaded\")\n",
    "    with open('../input/ecg-reg-best-model/test_samples.pickle', 'rb') as handle:\n",
    "        test_samples = pickle.load(handle)\n",
    "    print(\"test samples loaded\")\n",
    "else:\n",
    "    train_samples,test_samples = \\\n",
    "        get_features(path,train_generator,valid_generator,test_generator, optimizer)\n",
    "\n",
    "if save_samples:\n",
    "    with open('train_samples.pickle', 'wb+') as handle:\n",
    "        pickle.dump(train_samples, handle)\n",
    "    with open('valid_samples.pickle', 'wb+') as handle:\n",
    "        pickle.dump(valid_samples, handle)\n",
    "    with open('test_samples.pickle', 'wb+') as handle:\n",
    "        pickle.dump(test_samples, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:51.465312Z",
     "iopub.status.busy": "2020-12-15T12:34:51.464327Z",
     "iopub.status.idle": "2020-12-15T12:34:52.260144Z",
     "shell.execute_reply": "2020-12-15T12:34:52.259558Z"
    },
    "papermill": {
     "duration": 0.823675,
     "end_time": "2020-12-15T12:34:52.260258",
     "exception": false,
     "start_time": "2020-12-15T12:34:51.436583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:52.316270Z",
     "iopub.status.busy": "2020-12-15T12:34:52.315611Z",
     "iopub.status.idle": "2020-12-15T12:34:58.478672Z",
     "shell.execute_reply": "2020-12-15T12:34:58.478066Z"
    },
    "papermill": {
     "duration": 6.193468,
     "end_time": "2020-12-15T12:34:58.478806",
     "exception": false,
     "start_time": "2020-12-15T12:34:52.285338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "cdef extern from \"complex.h\":\n",
    "    double exp(double complex)\n",
    "cdef extern from \"<math.h>\":\n",
    "    double tanh(double complex)\n",
    "    double sqrt(double complex)\n",
    "    \n",
    "def polynomialKernel(double[:] x, double[:] y):\n",
    "    cdef int n = x.shape[0]\n",
    "    cdef double res = 0.\n",
    "    for i in range(n):\n",
    "        res += (x[i] * y[i])\n",
    "    res = res/n + 1.\n",
    "    return exp(-(res**4))\n",
    "\n",
    "def cosKernel(double[:] x, double[:] y):\n",
    "    cdef int n = x.shape[0]\n",
    "    cdef double res = 0.\n",
    "    cdef double sumx = 0., sumy = 0.\n",
    "    for i in range(n):\n",
    "        res += (x[i] * y[i])\n",
    "        sumx += (x[i]**2)\n",
    "        sumy += (y[i]**2)\n",
    "    res /= (sqrt(sumx*sumy))\n",
    "    return 1-res\n",
    "\n",
    "def sigmoidKernel(double[:] x, double[:] y):\n",
    "    cdef int n = x.shape[0]\n",
    "    cdef double res = 0.\n",
    "    for i in range(n):\n",
    "        res += (x[i] * y[i])\n",
    "    res = res/n + 1.\n",
    "    return 1 - tanh(res)\n",
    "\n",
    "def gaussianKernel(double[:] x, double[:] y):\n",
    "    cdef int n = x.shape[0]\n",
    "    cdef double res = 0.\n",
    "    for i in range(n):\n",
    "        res += (x[i] - y[i])**2\n",
    "    res *= -1./n\n",
    "    return 1 - exp(res)\n",
    "\n",
    "def chi2Kernel(double[:] x, double[:] y):\n",
    "    cdef int n = x.shape[0]\n",
    "    cdef double res = 0.\n",
    "    for i in range(n):\n",
    "        res += ((x[i] - y[i])**2 / (x[i] + y[i] + 1e-8))\n",
    "    return 1 - exp(-res)\n",
    "\n",
    "def l2Dist(double[:] x, double[:] y):\n",
    "    cdef int n = x.shape[0]\n",
    "    cdef double res = 0\n",
    "    cdef double resbis = 0\n",
    "    for i in range(n):\n",
    "        res += (x[i] - y[i])**2\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:34:58.541869Z",
     "iopub.status.busy": "2020-12-15T12:34:58.540932Z",
     "iopub.status.idle": "2020-12-15T12:35:00.502254Z",
     "shell.execute_reply": "2020-12-15T12:35:00.502997Z"
    },
    "papermill": {
     "duration": 1.998944,
     "end_time": "2020-12-15T12:35:00.503157",
     "exception": false,
     "start_time": "2020-12-15T12:34:58.504213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn builded, cost 0.016984224319458008 second\n",
      "test got, cost 1.5587034225463867 second\n",
      "cost 1.575920581817627 time in total\n"
     ]
    }
   ],
   "source": [
    "#k nearest training\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=topk, algorithm='brute', metric='braycurtis')\n",
    "#nbrs = NearestNeighbors(n_neighbors=topk, algorithm='brute', metric='euclidean')\n",
    "nbrs.fit(np.stack((train_samples[\"dynamic_features\"])))\n",
    "\n",
    "build_time = time.time()\n",
    "print(\"knn builded, cost %s second\" % str(build_time-start))\n",
    "\n",
    "test_distances, test_indices = nbrs.kneighbors(np.stack(test_samples[\"dynamic_features\"]).astype('double'), n_neighbors=topk)\n",
    "test_time = time.time()\n",
    "print(\"test got, cost %s second\" % str(test_time-build_time))\n",
    "print(\"cost %s time in total\" % str(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:35:00.560881Z",
     "iopub.status.busy": "2020-12-15T12:35:00.560220Z",
     "iopub.status.idle": "2020-12-15T12:35:30.791892Z",
     "shell.execute_reply": "2020-12-15T12:35:30.792372Z"
    },
    "papermill": {
     "duration": 30.263126,
     "end_time": "2020-12-15T12:35:30.792580",
     "exception": false,
     "start_time": "2020-12-15T12:35:00.529454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dtaidistance\r\n",
      "  Downloading dtaidistance-2.1.2.tar.gz (752 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 752 kB 886 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: cython in /opt/conda/lib/python3.7/site-packages (from dtaidistance) (0.29.21)\r\n",
      "Building wheels for collected packages: dtaidistance\r\n",
      "  Building wheel for dtaidistance (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for dtaidistance: filename=dtaidistance-2.1.2-cp37-cp37m-linux_x86_64.whl size=1530980 sha256=f3520b69df5103e85ab9d7c318f1d9433f9a35a17dd7875c13fe513d40cca39c\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/db/36/ce/5db51c9513fe6438475bda5419521648c70a81165b9f0fdaed\r\n",
      "Successfully built dtaidistance\r\n",
      "Installing collected packages: dtaidistance\r\n",
      "Successfully installed dtaidistance-2.1.2\r\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.3.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#!pip install fastdtw\n",
    "#!pip install dtw\n",
    "!pip install dtaidistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:35:30.878847Z",
     "iopub.status.busy": "2020-12-15T12:35:30.878057Z",
     "iopub.status.idle": "2020-12-15T12:35:30.934149Z",
     "shell.execute_reply": "2020-12-15T12:35:30.933615Z"
    },
    "papermill": {
     "duration": 0.104308,
     "end_time": "2020-12-15T12:35:30.934283",
     "exception": false,
     "start_time": "2020-12-15T12:35:30.829975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "filterAllZeroLabel = False\n",
    "use_0_1_relevant = False\n",
    "use_F1_relevant = True\n",
    "use_F1_corr_relevant = True\n",
    "\n",
    "# compute dcg@k for a single sample\n",
    "def dcg_at_k(r, k):\n",
    "    # k should be <= r.size()\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        #return r[0] + np.sum(r[1:] / np.log2(np.arange(3, r.size + 2)))\n",
    "        return np.sum((2**r-1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "\n",
    "# compute ndcg@k (dcg@k / idcg@k) for a single sample\n",
    "def get_ndcg(r, ref, k):\n",
    "    dcg_max = dcg_at_k(ref, k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    dcg = dcg_at_k(r, k)\n",
    "    return dcg / dcg_max\n",
    "\n",
    "def get_pred_vec(pre_label, anchor_label):\n",
    "    pass\n",
    "\n",
    "\"\"\"def normCorrelation(sample1, sample2):\n",
    "    corr = []\n",
    "    for a, b in zip(sample1[\"index_data\"], sample2[\"index_data\"]):\n",
    "        a = (a - np.mean(a)) / (np.std(a) * len(a))\n",
    "        b = (b - np.mean(b)) / (np.std(b))\n",
    "        corr.append(np.correlate(a, b, 'valid'))\n",
    "    return sum(corr) / len(corr)\"\"\"\n",
    "\n",
    "def KLDist(sample1, sample2):\n",
    "    kl_d = []\n",
    "    for a, b in zip(sample1[\"index_data\"], sample2[\"index_data\"]):\n",
    "        a = [item if item != 0 else 0.01 for item in a]\n",
    "        b = [item if item != 0 else 0.01 for item in b]\n",
    "        a = a / np.sum(a)\n",
    "        b = b / np.sum(b)\n",
    "        assert sum(a) - 1. < 1e-3 and sum(b) - 1. < 1e-3\n",
    "        #v = scipy.stats.entropy(a,b)\n",
    "        v = np.sum(np.where(a != 0, a * np.log(a / b), 0))\n",
    "        v = 0 if np.isfinite(v) == False else v\n",
    "        kl_d.append(abs(v))\n",
    "    return sum(kl_d) / len(kl_d)\n",
    "    \n",
    "def plotSignal(signal):\n",
    "    plt.plot(signal)\n",
    "    plt.show()\n",
    "    \n",
    "def normCorrelation(sample1, sample2):\n",
    "    corr = []\n",
    "    for a, b in zip(sample1[\"index_data\"], sample2[\"index_data\"]):\n",
    "        \"\"\"a = np.array(normalize(a))\n",
    "        b = np.array(normalize(b))\n",
    "        a = (a - np.mean(a)) / (np.std(a))\n",
    "        b = (b - np.mean(b)) / (np.std(b))\"\"\"\n",
    "        v = np.corrcoef(a, b)[0][1]\n",
    "        \"\"\"a = (a - np.mean(a)) / (np.std(a) * len(a))\n",
    "        b = (b - np.mean(b)) / (np.std(b))\n",
    "        #v = np.correlate(a, b, 'valid')\n",
    "        v = np.correlate(a, b, 'valid')[0]\"\"\"\n",
    "        \"\"\"if v > 0. and v < 0.05:\n",
    "            print(a)\n",
    "            print(b)\n",
    "            plotSignal(a)\n",
    "            plotSignal(b)\"\"\"\n",
    "        v = 0 if np.isfinite(v) == False else v\n",
    "        corr.append(abs(v))\n",
    "    return sum(corr) / len(corr)\n",
    "\n",
    "def normMI(sample1, sample2):\n",
    "    nmi = []\n",
    "    for a, b in zip(sample1[\"index_data\"], sample2[\"index_data\"]):\n",
    "        v = sklearn.metrics.normalized_mutual_info_score(a, b)\n",
    "        v = 0 if np.isfinite(v) == False else v\n",
    "        nmi.append(abs(v))\n",
    "    return sum(nmi) / len(nmi)\n",
    "\n",
    "def nrmseDist(sample1, sample2):\n",
    "    rms = []\n",
    "    for a, b in zip(sample1[\"index_data\"], sample2[\"index_data\"]):\n",
    "        #a = (a - np.mean(a)) / (np.std(a))\n",
    "        #b = (b - np.mean(b)) / (np.std(b))\n",
    "        a = np.array(normalize(a))\n",
    "        b = np.array(normalize(b))\n",
    "        #mean_squared_error(normalize(a), normalize(b))\n",
    "        v = np.sqrt(((a - b) ** 2).mean())\n",
    "        #assert v <= 1.0 and v >= 0.0\n",
    "        v = 0 if np.isfinite(v) == False else v\n",
    "        rms.append(abs(v))\n",
    "    return sum(rms)/len(rms)\n",
    "\n",
    "def cosDist(sample1, sample2):\n",
    "    cos = []\n",
    "    for a, b in zip(sample1[\"index_data\"], sample2[\"index_data\"]):\n",
    "        v = 1 - scipy.spatial.distance.cosine(a,b)\n",
    "        v = 0 if np.isfinite(v) == False else v\n",
    "        cos.append(abs(v))\n",
    "    return sum(cos)/len(cos)\n",
    "\n",
    "def corr_score(train_index, test_index):\n",
    "    test_add = test_samples[\"file_name\"][test_index]\n",
    "    train_add = train_samples[\"file_name\"][train_index]\n",
    "    sample1 = parse_line(train_add)\n",
    "    sample2 = parse_line(test_add)\n",
    "    no = normCorrelation(sample1, sample2)\n",
    "    return no\n",
    "    \n",
    "def getIdeal(test_index):\n",
    "    test_add = test_samples[\"file_name\"][test_index]\n",
    "    name = test_add[test_add.rindex('/')+1:]\n",
    "    return np.array(corr_dict[name], dtype=np.float32), \\\n",
    "            np.array(f1_dict[name], dtype=np.float32)\n",
    "\n",
    "def load_corr_dict(add):\n",
    "    corr_dict = {}\n",
    "    with open(add) as f:\n",
    "        for line in f:\n",
    "            data = line.strip().split(\"\\t\")\n",
    "            data = [item if item != \".\" else \"0.0\" for item in data]\n",
    "            name = data[0]\n",
    "            s1, s2, s3, s4, s5 = data[1:]\n",
    "            #corr_dict[name] = np.array([s1, s1, s2, s2, s3, s4, s5])\n",
    "            corr_dict[name] = np.array([s1, s1, s2, s2, s3, s3, s4, s4, s5, s5])\n",
    "    return corr_dict\n",
    "  \n",
    "def load_f1_dict(add):\n",
    "    f1_dict = {}\n",
    "    with open(add) as f:\n",
    "        for line in f:\n",
    "            data = line.strip().split(\"\\t\")\n",
    "            name = data[0]\n",
    "            f1_dict[name] = np.array(data[1:])\n",
    "    return f1_dict\n",
    "\n",
    "count_labels = [0] * 123\n",
    "labels_scores_r1 = [0.] * num_class\n",
    "labels_scores_r5 = [0.] * num_class\n",
    "labels_scores_r10 = [0.] * num_class\n",
    "def retrieval_eval_ndcg(indices, test_distances, k):\n",
    "    recalls = []\n",
    "    se_any = 0\n",
    "    se_all = 0\n",
    "    se_all_valid = 0\n",
    "    correct_pred = 0\n",
    "    valid_test_cases = 0\n",
    "    smooth = [1.0] * 100\n",
    "    for i in range(1,100):\n",
    "        smooth[i] = smooth[i-1] * 2.8/5\n",
    "    #print(smooth)\n",
    "    for q,candidates in enumerate(indices):\n",
    "        if len(candidates) == 0: continue\n",
    "        valid_test_cases += 1\n",
    "        candidates = candidates[:k]\n",
    "        q_id = test_samples['id'][q]\n",
    "        total_segment = int(test_samples['segment'][q].split('/')[-1]) - num_test[q_id]   #except test query itself\n",
    "        hit_list = [1 if train_samples['id'][p]==test_samples['id'][q] else 0 for p in candidates]\n",
    "        #if k == 10: print(q, hit_list, candidates)\n",
    "        \n",
    "        #compute EER\n",
    "        pred_list = [train_samples['id'][p] for p in candidates[:total_segment]]\n",
    "        score_dict = {}\n",
    "        hit_num_dict = {}\n",
    "        for c, p in enumerate(candidates):\n",
    "            id = train_samples['id'][p]\n",
    "            try:\n",
    "                #score_dict[id] = score_dict.get(id,0.) + math.exp(-test_distances[q][c])\n",
    "                train_segment = int(train_samples['segment'][p].split('/')[-1]) - 1\n",
    "                score_dict[id] = score_dict.get(id,0.) + (1 / test_distances[q][c]) * (1+50)/(train_segment+50)\n",
    "                hit_num_dict[id] = hit_num_dict.get(id,0) + 1\n",
    "                #score_dict[id] = score_dict.get(id,0.) + (1-test_distances[q][c])**30\n",
    "                #score_dict[id] = score_dict.get(id,0.) + 1-test_distances[q][c]\n",
    "            except:\n",
    "                print(candidates, test_distances[q])\n",
    "                pass\n",
    "        #if k == 10 or k == 20: print(candidates,score_dict)\n",
    "        #for id in score_dict: score_dict[id] *= hit_num_dict[id]\n",
    "        #prediction = max(score_dict, key=score_dict.get)    #by dist score dict\n",
    "        #prediction = max(set(pred_list), key=pred_list.count)   #by simple voting\n",
    "        prediction = train_samples['id'][candidates[0]]   #by first one\n",
    "        \"\"\"if k == 10: \n",
    "            print(test_samples['id'][q], hit_list, pred_list, prediction)\n",
    "            print(score_dict)\"\"\"\n",
    "        if prediction == test_samples['id'][q]: correct_pred += 1\n",
    "        #elif k == 20: print(test_samples['id'][q], total_segment, '\\n', hit_list, '\\n', test_distances[q][:k], '\\n', score_dict, '\\n', prediction)\n",
    "            \n",
    "        count = sum(hit_list)\n",
    "        assert count <= total_segment\n",
    "        #recall = count/min(total_segment, len(candidates))\n",
    "        recall = count/total_segment\n",
    "        recalls.append(recall)\n",
    "        #if k == 10: print(hit_list)\n",
    "        #print(hit_list, total_segment, recall)\n",
    "        if count != 0: se_any += 1\n",
    "        \"\"\"if total_segment <= k:\n",
    "            se_all_valid += 1\n",
    "            #if k == 10: print(count, total_segment)\n",
    "            if count == total_segment:\n",
    "                se_all += 1\"\"\"\n",
    "        if count == total_segment:\n",
    "            se_all += 1\n",
    "        se_all_valid += 1\n",
    "    try:\n",
    "        print(\"k=\",k, \" recall: \",sum(recalls) / len(recalls), \" se any: \",se_any/valid_test_cases, \" se all: \",se_all/se_all_valid, \"correct pred: \",correct_pred/valid_test_cases) \n",
    "    except:\n",
    "        print(\"k=\",k, \" recall: \",sum(recalls) / len(recalls), \" se any: \",se_any/valid_test_cases,  \" se all: invalid\", \"correct pred: \",correct_pred/valid_test_cases) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:35:31.023757Z",
     "iopub.status.busy": "2020-12-15T12:35:31.023064Z",
     "iopub.status.idle": "2020-12-15T12:35:31.595218Z",
     "shell.execute_reply": "2020-12-15T12:35:31.594416Z"
    },
    "papermill": {
     "duration": 0.624362,
     "end_time": "2020-12-15T12:35:31.595345",
     "exception": false,
     "start_time": "2020-12-15T12:35:30.970983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1  recall:  0.08192306310893356  se any:  0.9834254143646409  se all:  0.0 correct pred:  0.9834254143646409\n",
      "k= 5  recall:  0.2957318285386208  se any:  0.992633517495396  se all:  0.07918968692449356 correct pred:  0.9834254143646409\n",
      "k= 10  recall:  0.4946362480712605  se any:  0.992633517495396  se all:  0.1712707182320442 correct pred:  0.9834254143646409\n",
      "k= 20  recall:  0.6530298918231466  se any:  0.994475138121547  se all:  0.3388581952117864 correct pred:  0.9834254143646409\n",
      "k= 30  recall:  0.7425920510917942  se any:  0.994475138121547  se all:  0.3996316758747698 correct pred:  0.9834254143646409\n",
      "k= 100  recall:  0.8863125145023953  se any:  1.0  se all:  0.6556169429097606 correct pred:  0.9834254143646409\n"
     ]
    }
   ],
   "source": [
    "retrieval_eval_ndcg(test_indices, test_distances, 1)\n",
    "retrieval_eval_ndcg(test_indices, test_distances, 5)\n",
    "retrieval_eval_ndcg(test_indices, test_distances, 10)\n",
    "retrieval_eval_ndcg(test_indices, test_distances, 20)\n",
    "retrieval_eval_ndcg(test_indices, test_distances, 30)\n",
    "retrieval_eval_ndcg(test_indices, test_distances, topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T12:35:31.683715Z",
     "iopub.status.busy": "2020-12-15T12:35:31.682978Z",
     "iopub.status.idle": "2020-12-15T14:14:44.263177Z",
     "shell.execute_reply": "2020-12-15T14:14:44.260708Z"
    },
    "papermill": {
     "duration": 5952.630012,
     "end_time": "2020-12-15T14:14:44.263306",
     "exception": false,
     "start_time": "2020-12-15T12:35:31.633294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea780a29e62440e988bfaa0817a4753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "rec  [0.65, 0.51]\n"
     ]
    }
   ],
   "source": [
    "#RERANKING\n",
    "from dtaidistance import dtw\n",
    "from scipy.spatial import distance\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "    \n",
    "def parse_line(f_add):\n",
    "    \n",
    "    with open(f_add, 'r') as f:\n",
    "        line = f.read()\n",
    "        id, name, segment, data, label, features = line.strip().split('\\t')\n",
    "        \n",
    "        index_data = base64.b64decode(data)\n",
    "        index_data = np.frombuffer(index_data).reshape(12,-1)\n",
    "        \n",
    "        index_label = base64.b64decode(label)\n",
    "        index_label = np.frombuffer(index_label)\n",
    "        \n",
    "        sample = {}\n",
    "        sample['id'] = id\n",
    "        sample['file_name'] = f_add\n",
    "        sample['index_name'] = name\n",
    "        sample['segment'] = segment\n",
    "        sample['index_data'] = index_data\n",
    "        sample['index_label'] = index_label\n",
    "        sample['features'] = features\n",
    "        \n",
    "        return sample\n",
    "\n",
    "def standardized(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "        return v\n",
    "    return v / norm\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum() # only difference\n",
    "\n",
    "def normalize(x):\n",
    "    x = np.asarray(x)\n",
    "    ma = np.max(x)\n",
    "    mi = np.min(x)\n",
    "    if ma == mi: \n",
    "        return x\n",
    "    #return [(item-mi)/(ma-mi) for item in x]\n",
    "    return (x - mi) / (ma - mi)\n",
    "\n",
    "def computeJaccardDist(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return 1 - float(intersection) / union\n",
    "\n",
    "def neighborsFix(neighbors_dist, neighbors_indices):\n",
    "    neighbors_dist_n = []\n",
    "    neighbors_indices_n = []\n",
    "    for i in range(len(neighbors_dist)):\n",
    "        if  neighbors_dist[i][0] == 0.:\n",
    "            neighbors_dist_n.append(neighbors_dist[i][1:])\n",
    "            neighbors_indices_n.append(neighbors_indices[i][1:])\n",
    "        else:\n",
    "            neighbors_dist_n.append(neighbors_dist[i][:-1])\n",
    "            neighbors_indices_n.append(neighbors_indices[i][0:-1])\n",
    "    return neighbors_dist_n, neighbors_indices_n\n",
    "\n",
    "def get_rec_neighbors(q_fea, k, nbrs, k2=100, ratio=1.0):\n",
    "    neighbors_dist, neighbors_indices = nbrs.kneighbors(q_fea.reshape(1,-1),k)\n",
    "    #neighbors_dist, neighbors_indices = neighborsFix(neighbors_dist, neighbors_indices)\n",
    "    neighbors_dist = neighbors_dist[0]\n",
    "    neighbors_indices = neighbors_indices[0]\n",
    "    assert len(neighbors_dist) == k\n",
    "    neighbors_fea = [train_samples[\"dynamic_features\"][i] for i in neighbors_indices]\n",
    "    \n",
    "    candidates_neighbors_dist = []\n",
    "    candidates_neighbors_indices = []\n",
    "    p_neighbors_dists, p_neighbors_indices = nbrs.kneighbors(np.stack(neighbors_fea), k2)\n",
    "    for i,p in enumerate(neighbors_indices):\n",
    "        total_segment = int(train_samples['segment'][p].split('/')[-1])   #except p itself\n",
    "        candidates_neighbors_dist.append(p_neighbors_dists[i][:int(total_segment*ratio)])\n",
    "        candidates_neighbors_indices.append(p_neighbors_indices[i][:int(total_segment*ratio)])\n",
    "        \n",
    "    #assert len(candidates_neighbors_dist[0]) == k2\n",
    "    #print(\"candidates_neighbors_dist: \", candidates_neighbors_dist)\n",
    "    rec_candidates = [neighbors_indices[i] for i in range(len(neighbors_indices)) if neighbors_dist[i] < candidates_neighbors_dist[i][-1]]\n",
    "    rec_dist = [neighbors_dist[i] for i in range(len(neighbors_dist)) if neighbors_dist[i] < candidates_neighbors_dist[i][-1]]\n",
    "    \n",
    "    #expand invalid set\n",
    "    \"\"\"if len(rec_candidates) == 0:\n",
    "        rec_candidates.append(neighbors_indices[0])\n",
    "        rec_dist.append(neighbors_dist[0])\"\"\"\n",
    "        \n",
    "    return neighbors_dist, neighbors_indices, rec_dist, rec_candidates\n",
    "\n",
    "def reciprocalDist(test_index, q_fea, candidates, nbrs, do_expand = True):\n",
    "    #filtering\n",
    "    ori_dist, ori_neighbors, q_rec_dist, q_rec_neighbors = get_rec_neighbors(q_fea, topk, nbrs)\n",
    "    \n",
    "    p_rec_dist_dict = {}\n",
    "    p_rec_neighbors_dict = {}\n",
    "    \n",
    "    p_corrs = []\n",
    "    for train_index in q_rec_neighbors:\n",
    "        p_corrs.append(corr_score(train_index, test_index))\n",
    "    p_corrs_avg = sum(p_corrs)/len(p_corrs) if p_corrs else 0.\n",
    "    #print(p_corrs, p_corrs_avg)\n",
    "    \n",
    "    for p in q_rec_neighbors.copy():\n",
    "        p_fea = train_samples[\"dynamic_features\"][p]\n",
    "        ori_p_dist, ori_p_neighbors, p_rec_dist, p_rec_neighbors = get_rec_neighbors(p_fea, topk, nbrs)\n",
    "        \n",
    "        p_rec_dist_dict[str(p)] = p_rec_dist\n",
    "        p_rec_neighbors_dict[str(p)] = p_rec_neighbors\n",
    "        \n",
    "        #expand candidate set\n",
    "        #if len(list(set(p_rec_neighbors).intersection(q_rec_neighbors))) >= 3./4.*len(p_rec_neighbors):\n",
    "        if do_expand and computeJaccardDist(p_rec_neighbors, q_rec_neighbors) <= 1./3.:\n",
    "           for pp in range(len(p_rec_neighbors)):\n",
    "                n_p = p_rec_neighbors[pp]\n",
    "                if n_p not in q_rec_neighbors:\n",
    "                    \n",
    "                    #corr examine\n",
    "                    if corr_score(n_p, test_index) < p_corrs_avg: continue\n",
    "                        \n",
    "                    #update new candidate\n",
    "                    q_rec_dist.append(p_rec_dist[pp])\n",
    "                    q_rec_neighbors.append(n_p)\n",
    "                    #update p_rec_dict\n",
    "                    pp_fea = train_samples[\"dynamic_features\"][n_p]\n",
    "                    ori_pp_dist, ori_pp_neighbors, pp_rec_dist, pp_rec_neighbors = \\\n",
    "                            get_rec_neighbors(pp_fea, topk, nbrs)\n",
    "                    p_rec_dist_dict[str(n_p)] = pp_rec_dist\n",
    "                    p_rec_neighbors_dict[str(n_p)] = pp_rec_neighbors\n",
    "    \n",
    "    #compute new distance\n",
    "    dist = []\n",
    "    v_q = np.asarray([0.] * num_samples)\n",
    "    for d,p in zip(q_rec_dist, q_rec_neighbors):\n",
    "        v_q[p] = math.exp(-d)\n",
    "    for p in p_rec_dist_dict:\n",
    "        p_rec_dist = p_rec_dist_dict[p]\n",
    "        p_rec_neighbors = p_rec_neighbors_dict[p]\n",
    "        v_p = np.asarray([0.] * num_samples)\n",
    "        #print(p_rec_dist, p_rec_neighbors)\n",
    "        for d,pp in zip(p_rec_dist, p_rec_neighbors):\n",
    "            #self\n",
    "            if d - 0.0 < 1e-6: continue\n",
    "            v_p[pp] = math.exp(-d)\n",
    "        intersection = np.sum(np.minimum(v_q, v_p))\n",
    "        union = np.sum(np.maximum(v_q, v_p))\n",
    "        #print(intersection, union)\n",
    "        dist.append(intersection/union)\n",
    "        #dist.append(-np.linalg.norm(v_q-v_p)/len(p_rec_dist))\n",
    "        \"\"\"v = scipy.spatial.distance.cosine(v_q, v_p)\n",
    "        v = 0 if np.isfinite(v) == False else v\n",
    "        dist.append(1-v)\"\"\"\n",
    "        \n",
    "    return q_rec_neighbors, dist\n",
    "\n",
    "def dtwDist(sample1, sample2):\n",
    "    #dist = [dtw.distance_fast(s1, s2) for s1,s2 in zip(sample1[\"index_data\"], sample2[\"index_data\"])]\n",
    "    #dist = normalize(dist)\n",
    "    dist = dtw.distance_fast(sample1[\"index_data\"][0], sample2[\"index_data\"][0])\n",
    "    return dist\n",
    "\n",
    "def stoDist(sample1, sample2):\n",
    "    #dist = np.linalg.norm(sample1[\"features\"] - sample2[\"features\"])\n",
    "    v = scipy.spatial.distance.cosine(sample1[\"features\"], sample2[\"features\"])\n",
    "    v = 0 if np.isfinite(v) == False else v\n",
    "    return v\n",
    "\n",
    "def reranking(indices, samples1, test_samples, nbrs, reranking_flag = False):\n",
    "    rec_dist_all = [0]*topk\n",
    "    \n",
    "    test_rec_indices = []\n",
    "    test_rec_dists = []\n",
    "    \n",
    "    count = 0\n",
    "    for test_index, indice in tqdm(enumerate(indices.copy())):\n",
    "        sample_add = test_samples[\"file_name\"][test_index]\n",
    "        \n",
    "        new_neighbors, new_dists = reciprocalDist(test_index, test_samples[\"dynamic_features\"][test_index], indice, nbrs)\n",
    "        \n",
    "        \"\"\"print(indice, test_distances[test_index])\n",
    "        print(new_neighbors, new_dists)\"\"\"\n",
    "        \n",
    "        rec_dist_all = [rec_dist_all[i] + new_dists[i] for i in range(min(len(new_dists),len(rec_dist_all)))]\n",
    "            \n",
    "        #reranking\n",
    "        if reranking_flag:\n",
    "            reorder = np.argsort(new_dists)[::-1]\n",
    "            #print(reorder, new_neighbors)\n",
    "            test_rec_indices.append([new_neighbors[i] for i in reorder])\n",
    "            test_rec_dists.append([new_dists[i] for i in reorder])\n",
    "        else:\n",
    "            test_rec_indices.append(new_neighbors)\n",
    "            test_rec_dists.append(new_dists)\n",
    "        #print(test_rec_dists[-1])\n",
    "        count += 1\n",
    "    \n",
    "    try:\n",
    "        print(\"rec \", [round(item / count, 2) for item in rec_dist_all])\n",
    "    except:\n",
    "        pass\n",
    "    return test_rec_indices, test_rec_dists\n",
    "\n",
    "use_reranking = True\n",
    "#valid_reranked_indices = reranking(valid_indices, train_samples, valid_samples, nbrs)\n",
    "if use_reranking:\n",
    "    test_rec_indices, test_rec_dist = reranking(test_indices, train_samples, test_samples, nbrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T14:14:44.344647Z",
     "iopub.status.busy": "2020-12-15T14:14:44.343953Z",
     "iopub.status.idle": "2020-12-15T14:14:44.615129Z",
     "shell.execute_reply": "2020-12-15T14:14:44.614392Z"
    },
    "papermill": {
     "duration": 0.31318,
     "end_time": "2020-12-15T14:14:44.615265",
     "exception": false,
     "start_time": "2020-12-15T14:14:44.302085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k= 1  recall:  0.08201514414024111  se any:  0.9871086556169429  se all:  0.0 correct pred:  0.9871086556169429\n",
      "k= 5  recall:  0.2959620311168896  se any:  0.992633517495396  se all:  0.07918968692449356 correct pred:  0.9871086556169429\n",
      "k= 10  recall:  0.49038972644323253  se any:  0.992633517495396  se all:  0.16206261510128914 correct pred:  0.9871086556169429\n",
      "k= 20  recall:  0.6440123701556297  se any:  0.994475138121547  se all:  0.30570902394106814 correct pred:  0.9871086556169429\n",
      "k= 30  recall:  0.7311124352960645  se any:  0.994475138121547  se all:  0.35911602209944754 correct pred:  0.9871086556169429\n",
      "k= 100  recall:  0.7924082706160832  se any:  0.994475138121547  se all:  0.4548802946593002 correct pred:  0.9871086556169429\n"
     ]
    }
   ],
   "source": [
    "retrieval_eval_ndcg(test_rec_indices, test_rec_dist, 1)\n",
    "retrieval_eval_ndcg(test_rec_indices, test_rec_dist, 5)\n",
    "retrieval_eval_ndcg(test_rec_indices, test_rec_dist, 10)\n",
    "retrieval_eval_ndcg(test_rec_indices, test_rec_dist, 20)\n",
    "retrieval_eval_ndcg(test_rec_indices, test_rec_dist, 30)\n",
    "retrieval_eval_ndcg(test_rec_indices, test_rec_dist, topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T14:14:44.700311Z",
     "iopub.status.busy": "2020-12-15T14:14:44.699632Z",
     "iopub.status.idle": "2020-12-15T14:14:44.702982Z",
     "shell.execute_reply": "2020-12-15T14:14:44.702295Z"
    },
    "papermill": {
     "duration": 0.047871,
     "end_time": "2020-12-15T14:14:44.703102",
     "exception": false,
     "start_time": "2020-12-15T14:14:44.655231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['file_name', 'id', 'index_name', 'index_data', 'index_label', 'segment', 'sto_features', 'dynamic_features'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T14:14:44.786959Z",
     "iopub.status.busy": "2020-12-15T14:14:44.786309Z",
     "iopub.status.idle": "2020-12-15T14:14:44.818249Z",
     "shell.execute_reply": "2020-12-15T14:14:44.818799Z"
    },
    "papermill": {
     "duration": 0.075577,
     "end_time": "2020-12-15T14:14:44.818951",
     "exception": false,
     "start_time": "2020-12-15T14:14:44.743374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4f76a9dad686>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T14:14:44.904830Z",
     "iopub.status.busy": "2020-12-15T14:14:44.904162Z",
     "iopub.status.idle": "2020-12-15T14:14:44.927695Z",
     "shell.execute_reply": "2020-12-15T14:14:44.928230Z"
    },
    "papermill": {
     "duration": 0.068224,
     "end_time": "2020-12-15T14:14:44.928405",
     "exception": false,
     "start_time": "2020-12-15T14:14:44.860181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score_0_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d25eca6077ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m for person, (a,b,c,d,e,f) in enumerate(zip(score_0_1, \\\n\u001b[0m\u001b[1;32m      3\u001b[0m                     score_0_1_dcg, score_f1, score_f1_dcg, score_corr, score_corr_dcg)):\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#if (a+b+c+d+e+f)/6 > 0.9 and (a+b+c+d+e+f)/6 < 1.0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'score_0_1' is not defined"
     ]
    }
   ],
   "source": [
    "candidates = []\n",
    "for person, (a,b,c,d,e,f) in enumerate(zip(score_0_1, \\\n",
    "                    score_0_1_dcg, score_f1, score_f1_dcg, score_corr, score_corr_dcg)):\n",
    "    #if (a+b+c+d+e+f)/6 > 0.9 and (a+b+c+d+e+f)/6 < 1.0:\n",
    "    if e < 0.4 and e > 0.3:\n",
    "        add1 = test_samples[\"file_name\"][person]\n",
    "        sample1 = parse_line(add1)\n",
    "        fea1 = sample1['features'][:2]\n",
    "        add2 = train_samples[\"file_name\"][test_indices[person][0]]\n",
    "        sample2 = parse_line(add2)\n",
    "        fea2 = sample2['features'][:2]\n",
    "        label = [i for i in range(num_class) if test_samples[\"index_label\"][person][i] == 1.]\n",
    "        if a == 1.0 and (fea1 != fea2).any():\n",
    "            print(person,a,b,c,d,e,f)\n",
    "            print(test_indices[person])\n",
    "            print(label)\n",
    "            candidates.append(person)\n",
    "#label = 632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T14:14:45.018736Z",
     "iopub.status.busy": "2020-12-15T14:14:45.017992Z",
     "iopub.status.idle": "2020-12-15T14:14:45.247601Z",
     "shell.execute_reply": "2020-12-15T14:14:45.247069Z"
    },
    "papermill": {
     "duration": 0.275963,
     "end_time": "2020-12-15T14:14:45.247723",
     "exception": false,
     "start_time": "2020-12-15T14:14:44.971760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-04291cfb406c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mnum_point\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m#draw_sample(valid_data[person])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mtest_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mdraw_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"index_data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_point\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'patient-'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#visualization\n",
    "# -*- coding:utf-8 -*-\n",
    "#绘制波形图\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"savefig.format\"] = 'png'\n",
    "\n",
    "# Create new figure and two subplots, sharing both axes\n",
    "def draw_sample(sample, num_point=1000, title=None):\n",
    "    \n",
    "    fig,((I,VR,V1,V4),(II,VL,V2,V5),(III,VF,V3,V6)) = plt.subplots(nrows=3, ncols=4, sharey=True, sharex=True, figsize=(30,15), frameon=False)\n",
    "    #fig.xlim = (0,num_point)\n",
    "    #fig.ylim = (-300,300)\n",
    "    I_WAVE, II_WAVE, III_WAVE, VL_WAVE, VR_WAVE, VF_WAVE, \\\n",
    "        V1_WAVE, V2_WAVE, V3_WAVE, V4_WAVE, V5_WAVE, V6_WAVE= sample\n",
    "    \"\"\"III_WAVE = II_WAVE-I_WAVE\n",
    "    VR_WAVE = -(I_WAVE+II_WAVE)/2\n",
    "    VL_WAVE = I_WAVE-II_WAVE/2\n",
    "    VF_WAVE = II_WAVE-I_WAVE/2\"\"\"\n",
    "\n",
    "    st = 0\n",
    "    ed = num_point\n",
    "    I.plot(I_WAVE[st:ed])\n",
    "    I.title.set_text('I')\n",
    "    II.plot(II_WAVE[st:ed])\n",
    "    II.title.set_text('II')\n",
    "    III.plot(III_WAVE[st:ed])\n",
    "    III.title.set_text('III')\n",
    "\n",
    "    VR.plot(VR_WAVE[st:ed])\n",
    "    VR.title.set_text('aVR')\n",
    "    VL.plot(VL_WAVE[st:ed])\n",
    "    VL.title.set_text('aVL')\n",
    "    VF.plot(VF_WAVE[st:ed])\n",
    "    VF.title.set_text('aVF')\n",
    "\n",
    "    V1.plot(V1_WAVE[st:ed])\n",
    "    V1.title.set_text('V1')\n",
    "    V2.plot(V2_WAVE[st:ed])\n",
    "    V2.title.set_text('V2')\n",
    "    V3.plot(V3_WAVE[st:ed])\n",
    "    V3.title.set_text('V3')\n",
    "    V4.plot(V4_WAVE[st:ed])\n",
    "    V4.title.set_text('V4')\n",
    "    V5.plot(V5_WAVE[st:ed])\n",
    "    V5.title.set_text('V5')\n",
    "    V6.plot(V6_WAVE[st:ed])\n",
    "    V6.title.set_text('V6')\n",
    "\n",
    "    fig.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.xlabel('Time:(1/500)s', fontsize=15)\n",
    "    #plt.ylabel('Amplitude', fontsize=15)\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, fontsize=15)\n",
    "        fig.savefig(title+'.jpg')\n",
    "    #fig.show()\n",
    "    \n",
    "person = 4210\n",
    "num_point=5000\n",
    "#draw_sample(valid_data[person])\n",
    "test_add = test_samples[\"file_name\"][person]\n",
    "sample = parse_line(test_add)\n",
    "draw_sample(sample[\"index_data\"], num_point, u'patient-'+str(person))\n",
    "for count, i in enumerate(test_indices[person]):\n",
    "    if count == 5: break\n",
    "    train_add = train_samples[\"file_name\"][i]\n",
    "    train_sample = parse_line(train_add)\n",
    "    draw_sample(train_sample[\"index_data\"], num_point, str(person)+'-relevant patient '+str(count+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T14:14:45.350206Z",
     "iopub.status.busy": "2020-12-15T14:14:45.344674Z",
     "iopub.status.idle": "2020-12-15T14:14:45.360793Z",
     "shell.execute_reply": "2020-12-15T14:14:45.361294Z"
    },
    "papermill": {
     "duration": 0.071425,
     "end_time": "2020-12-15T14:14:45.361445",
     "exception": false,
     "start_time": "2020-12-15T14:14:45.290020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ecg_plot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-291f8f78290b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#!pip install ecg-plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mecg_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mperson\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4210\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ecg_plot'"
     ]
    }
   ],
   "source": [
    "#!pip install ecg-plot\n",
    "import ecg_plot\n",
    "person = 4210\n",
    "times = 1500\n",
    "columns = 2\n",
    "row_height = 4\n",
    "test_add = test_samples[\"file_name\"][person]\n",
    "sample = parse_line(test_add)\n",
    "ecg_plot.plot(sample[\"index_data\"]*times, sample_rate = 500, title = u'Anchor', columns=columns, row_height=row_height, show_separate_line=False)\n",
    "ecg_plot.save_as_png('Anchor')\n",
    "#ecg_plot.show()\n",
    "print('label: ', [i for i in range(num_class) if sample[\"index_label\"][i] == 1.])\n",
    "print(\"feature: \", sample['features'][:2])\n",
    "for count, i in enumerate(test_indices[person]):\n",
    "    if count == 5: break\n",
    "    train_add = train_samples[\"file_name\"][i]\n",
    "    train_sample = parse_line(train_add)\n",
    "    ecg_plot.plot(train_sample[\"index_data\"]*times, sample_rate = 500, title = 'Relevant patient '+str(count+1), columns=columns, row_height=row_height, show_separate_line=False)\n",
    "    ecg_plot.save_as_png('Relevant patient '+str(count+1))\n",
    "    ecg_plot.show()\n",
    "    print('label: ', [l for l in range(num_class) if train_sample['index_label'][l] == 1.])\n",
    "    print(\"feature: \", train_sample['features'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T14:14:45.458350Z",
     "iopub.status.busy": "2020-12-15T14:14:45.452295Z",
     "iopub.status.idle": "2020-12-15T14:14:46.233896Z",
     "shell.execute_reply": "2020-12-15T14:14:46.233255Z"
    },
    "papermill": {
     "duration": 0.829266,
     "end_time": "2020-12-15T14:14:46.234037",
     "exception": false,
     "start_time": "2020-12-15T14:14:45.404771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.044354,
     "end_time": "2020-12-15T14:14:46.323472",
     "exception": false,
     "start_time": "2020-12-15T14:14:46.279118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 6004.683915,
   "end_time": "2020-12-15T14:14:46.476884",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-15T12:34:41.792969",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "507c4ccd8fee4ac88869c0147fc4ba10": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "89e940601a064766aab90d58df31a624": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9cd7f55c1c1f420ebbc8ac189f1239fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b9bb63cf3ce94ecfbfed580212fee7de",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_507c4ccd8fee4ac88869c0147fc4ba10",
       "value": 1.0
      }
     },
     "b9bb63cf3ce94ecfbfed580212fee7de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d51caedd331f49b1a79044a1d2f90bd5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dea780a29e62440e988bfaa0817a4753": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9cd7f55c1c1f420ebbc8ac189f1239fd",
        "IPY_MODEL_eb1aea4f63b0446b88bfe66379772298"
       ],
       "layout": "IPY_MODEL_dfa7088f825f407a94dfeb8ffab623b1"
      }
     },
     "dfa7088f825f407a94dfeb8ffab623b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "eb1aea4f63b0446b88bfe66379772298": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_89e940601a064766aab90d58df31a624",
       "placeholder": "​",
       "style": "IPY_MODEL_d51caedd331f49b1a79044a1d2f90bd5",
       "value": " 543/? [1:39:13&lt;00:00, 10.96s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
